# SnoopDeepWeb
This project is about the automated web crawling with the stormcrawler.
It's an automated crawler which can get the html source code and screenshot of the webpage in base64 formate
using stormcrawler running in local mode in Apache Storm.

# Tech Stack Used
* Apache Storm
* Stormcrawler
* Selenium
* Java
* MySQL Workbench
* Chrome

# Resources (from where I have learned)
* Edureka Youtube Playlist for Apache Storm and Stormcrawler
* Intellipaat Youtube Video for Selenium (for beginners)
* Selenium official documentation

# Steps for Deployment
This project contains all the dependencies in pom.xml which are need for running it. For running it in your system you can just clone it and resolve the dependencies provided
in pom.xml using Maven.
## Dependencies used :-
           javax.persistence-api
           mysql-connector-java
           apache.storm
           selenium-java    
           storm-crawler-core
           codahale.metrics  
           fasterxml.jackson.core

## Prepare a database on MySQL Workbench for storing data a
![image](https://user-images.githubusercontent.com/117909106/226212533-b3bb104c-a146-4a22-acc8-2560fde9c0ff.png)

# Key Takeaways
Got to learn about Apache Storm, Stormcrawler and Selenium. It's quite fascinating to see that Storm by Apache Foundation provides so much scalability and low latency
for the development of highly scalable crawlers. Also learnt about Selenium and it's benefits in testing a program.
